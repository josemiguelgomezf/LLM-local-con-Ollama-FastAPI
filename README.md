# LLM local con Ollama + FastAPI

Proyecto para ejecutar un LLM local usando **Ollama (Mistral)** con un backend en **FastAPI** y un frontend web sencillo en HTML/JS.

## ğŸš€ CaracterÃ­sticas
- LLM ejecutÃ¡ndose localmente con Ollama
- Backend FastAPI estable (no bloqueante)
- Frontend web minimalista
- ComunicaciÃ³n vÃ­a HTTP
- Pensado para VPS sin GPU

## ğŸ§  Stack
- Python 3.12
- FastAPI
- Ollama
- Mistral
- HTML + JavaScript

---

## ğŸ“¦ Requisitos
- Linux (probado en Ubuntu VPS)
- Python 3.10+
- 12 GB RAM recomendado
- Swap habilitado
- Ollama instalado

---

## âš™ï¸ InstalaciÃ³n

### 1ï¸âƒ£ Instalar Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral
2ï¸âƒ£ Crear entorno virtual
bash
Copiar cÃ³digo
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn
3ï¸âƒ£ Backend
bash
Copiar cÃ³digo
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000
4ï¸âƒ£ Frontend
bash
Copiar cÃ³digo
cd web
python3 -m http.server 80
Abre en el navegador:

cpp
Copiar cÃ³digo
http://IP_DEL_SERVIDOR
ğŸ“¡ Endpoint
POST /ask
json
Copiar cÃ³digo
{
  "question": "Hola, Â¿quiÃ©n eres?"
}
Respuesta:

json
Copiar cÃ³digo
{
  "answer": "..."
}
ğŸ› ï¸ Problemas comunes
Si tarda mucho â†’ comprobar swap

Si no responde â†’ Ollama no estÃ¡ warm

Si hay errores â†’ revisar procesos Ollama

Ver docs/problemas-soluciones.md
