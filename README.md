# LLM local con Ollama + FastAPI

Proyecto para ejecutar un LLM local usando **Ollama (Mistral)** con un backend en **FastAPI** y un frontend web sencillo en HTML/JS.

## üöÄ Caracter√≠sticas
- LLM ejecut√°ndose localmente con Ollama
- Backend FastAPI estable (no bloqueante)
- Frontend web minimalista
- Comunicaci√≥n v√≠a HTTP
- Pensado para VPS sin GPU

## üß† Stack
- Python 3.12
- FastAPI
- Ollama
- Mistral
- HTML + JavaScript

---

## üì¶ Requisitos
- Linux (probado en Ubuntu VPS)
- Python 3.10+
- 12 GB RAM recomendado
- Swap habilitado
- Ollama instalado

---

‚öôÔ∏è Instalaci√≥n y ejecuci√≥n (paso a paso)

Esta gu√≠a permite replicar el proyecto tal y como est√° funcionando actualmente, usando Ollama + FastAPI + frontend web simple.

1Ô∏è‚É£ Instalar Ollama y descargar el modelo
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral

‚ö†Ô∏è Importante: Ollama debe quedar ejecut√°ndose como servicio.
Si ollama serve ya est√° activo, no lo lances de nuevo (usa el puerto 11434).

Comprueba que responde:

curl http://localhost:11434/api/tags
2Ô∏è‚É£ Crear entorno virtual de Python

Desde la ra√≠z del proyecto:

python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn

‚úÖ Probado con Python 3.12

3Ô∏è‚É£ Ejecutar el backend (FastAPI)

El backend expone un endpoint /ask que env√≠a las preguntas al LLM v√≠a Ollama.

uvicorn main:app --host 0.0.0.0 --port 8000

Comprueba que est√° activo:

http://IP_DEL_SERVIDOR:8000/docs
4Ô∏è‚É£ Ejecutar el frontend web

En otra terminal, desde la carpeta donde est√° index.html:

python3 -m http.server 80

Abre en el navegador:

http://IP_DEL_SERVIDOR
üì° Uso de la API
Endpoint

POST /ask

Request
{
  "question": "Hola, ¬øqui√©n eres?"
}
Response
{
  "answer": "..."
}
üß† Notas importantes de funcionamiento

La primera pregunta puede tardar varios segundos (cold start del modelo).

Las siguientes preguntas son m√°s r√°pidas.

El backend es no bloqueante, pero el modelo consume bastante RAM.

Recomendado activar swap en VPS sin GPU.

üõ†Ô∏è Problemas comunes

‚è≥ Tarda mucho en responder
‚Üí El modelo est√° inicializando o no hay swap suficiente.

‚ùå Error conectando con el backend
‚Üí FastAPI no est√° levantado o el puerto 8000 no es accesible.

‚ùå Ollama no responde
‚Üí Verifica que el servicio est√° activo y escuchando en 127.0.0.1:11434.

Consulta m√°s detalles en docs/problemas-soluciones.md
