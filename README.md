# LLM-local-con-Ollama-FastAPI
Proyecto para ejecutar un LLM local usando **Ollama (Mistral)** con un backend en **FastAPI** y un frontend web sencillo en HTML/JS.
